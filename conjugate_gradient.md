# Conjugate Gradient Method

## Why Gradient Descent Isn't Enough

Gradient descent on a quadratic function f(x) = (1/2)x^T Ax - b^T x has a problem: it zigzags. Each step minimizes f along the gradient direction, but then the next step often cancels part of that progress because the new gradient points in a nearly perpendicular direction.

For an ill-conditioned matrix (i.e has a large condition number Œ∫), you need O(Œ∫) iterations to converge. That's... not great.


Conjugate gradient (CG) corrects this by choosing *smarter* search directions. Instead of repeatedly descending along gradients, we descend along **A-conjugate** (or A-orthogonal) directions. The payoff: convergence in at most n steps for an n√ón system, and O(‚àöŒ∫) iterations in practice.

## The Central Idea: A-Conjugacy

Two vectors p and q are A-conjugate (or A-orthogonal) if:

```
p^T A q = 0
```

This is like orthogonality, but with respect to the inner product ‚ü®p,q‚ü©_A = p^T A q instead of the standard inner product p^T q.

Why does this matter? Suppose we have n mutually A-conjugate search directions p‚ÇÄ, p‚ÇÅ, ..., p_{n-1}. Then we can write the exact solution as:

```
x* = Œ£·µ¢ Œ±·µ¢ p·µ¢
```

for some coefficients Œ±·µ¢. And here's the magic trick: we can compute each Œ±·µ¢ independently by minimizing f along that direction.

## The Algorithm (High Level)

Starting from x‚Å∞ = 0 (or any initial guess) and r‚Å∞ = b - Ax‚Å∞:

```
1. Set p‚Å∞ = r‚Å∞ (first search direction is the gradient)
2. For k = 0, 1, 2, ...
   a. Step size: Œ±‚Çñ = (r·µè^T r·µè) / (p·µè^T A p·µè)
   b. Update solution: x^(k+1) = x·µè + Œ±‚Çñ p·µè
   c. Update residual: r^(k+1) = r·µè - Œ±‚Çñ A p·µè
   d. If converged, stop
   e. Improvement direction: Œ≤‚Çñ = (r^(k+1)^T r^(k+1)) / (r·µè^T r·µè)
   f. New search direction: p^(k+1) = r^(k+1) + Œ≤‚Çñ p·µè
```

The directions p‚Å∞, p¬π, p¬≤, ... are automatically A-conjugate. That's not obvious at all, but it falls out of the math.

## Derivation: Where Do These Formulas Come From?

Let's derive the CG algorithm from scratch. We want to minimize f(x) along direction p·µè starting from x·µè:

```
min_Œ± f(x·µè + Œ± p·µè)
```

Taking the derivative with respect to Œ± and setting it to zero:

```
d/dŒ± f(x·µè + Œ± p·µè) = ‚àáf(x·µè + Œ± p·µè)^T p·µè = 0
```

At the optimum Œ± = Œ±‚Çñ:

```
‚àáf(x^(k+1))^T p·µè = 0
(A x^(k+1) - b)^T p·µè = 0
```

Since the residual r^(k+1) = b - A x^(k+1), this says:

```
(r^(k+1))^T p·µè = 0
```

So the new residual is orthogonal to the search direction. Cool.

Now, how do we choose the next search direction p^(k+1)? We want it to be A-conjugate to all previous directions. The clever insight: take p^(k+1) to be the residual r^(k+1) plus a correction that makes it A-orthogonal to p·µè:

```
p^(k+1) = r^(k+1) - Œ≤‚Çñ p·µè
```

To find Œ≤‚Çñ, impose A-conjugacy:

```
(p^(k+1))^T A p·µè = 0
(r^(k+1) - Œ≤‚Çñ p·µè)^T A p·µè = 0
Œ≤‚Çñ = (r^(k+1)^T A p·µè) / (p·µè^T A p·µè)
```

But we can simplify this. Since r^(k+1) = r·µè - Œ±‚Çñ A p·µè:

```
r^(k+1)^T A p·µè = (r·µè - Œ±‚Çñ A p·µè)^T A p·µè
               = r·µè^T A p·µè - Œ±‚Çñ (A p·µè)^T A p·µè
```

Now, from the definition of Œ±‚Çñ and the fact that r·µè^T p·µè = r·µè^T r·µè (I'm skipping some algebra here, but you can verify this using p·µè = r·µè + Œ≤‚Çñ‚Çã‚ÇÅ p^(k-1) and orthogonality of residuals), we get:

```
r·µè^T A p·µè = (r·µè^T r·µè) / Œ±‚Çñ
```

Substituting back:

```
r^(k+1)^T A p·µè = (r·µè^T r·µè) / Œ±‚Çñ - (r·µè^T r·µè) = -Œ≤‚Çñ (p·µè^T A p·µè) (r·µè^T r·µè) / Œ±‚Çñ
```

Wait, this is getting messy. Let me use a different approach.

Actually, there's a cleaner formula that you can derive using the conjugacy relations:

```
Œ≤‚Çñ = (r^(k+1)^T r^(k+1)) / (r·µè^T r·µè)
```

This is the Fletcher-Reeves formula. It's equivalent to the one above but only requires residual norms, not matrix-vector products.

## Why CG Works: Krylov Subspaces

Here's the deeper theory. Define the Krylov subspace:

```
ùí¶‚Çñ = span{r‚Å∞, Ar‚Å∞, A¬≤r‚Å∞, ..., A^(k-1)r‚Å∞}
```

CG has the property that:
1. The iterate x·µè lies in the affine subspace x‚Å∞ + ùí¶‚Çñ
2. x·µè minimizes f over that subspace
3. The residual r·µè is orthogonal to ùí¶‚Çñ

This is huge. It means CG is implicitly building an orthonormal basis for ùí¶‚Çñ (the search directions) and solving a small k√ók optimization problem at each step.

Since ùí¶‚Çô = ‚Ñù‚Åø for an n√ón matrix, CG finds the exact solution in at most n steps (in exact arithmetic). In practice, rounding errors screw this up, but you still converge quickly.

## Convergence Rate

In practice, CG converges long before n iterations. The error decreases as:

```
||x* - x·µè||_A ‚â§ 2 ((‚àöŒ∫ - 1)/(‚àöŒ∫ + 1))^k ||x* - x‚Å∞||_A
```

where ||z||_A = ‚àö(z^T A z) is the A-norm and Œ∫ = Œª_max/Œª_min is the condition number.

Compare to gradient descent: O(Œ∫) iterations ‚Üí CG: O(‚àöŒ∫) iterations. For Œ∫ = 10,000 that's 10,000 vs. 100 iterations. That's why people actually use CG.

## Preconditioning

If A is badly conditioned, even O(‚àöŒ∫) can be too slow. The solution: preconditioning.

Instead of solving Ax = b, solve:

```
M‚Åª¬π A x = M‚Åª¬π b
```

where M ‚âà A but M‚Åª¬π is cheap to compute. If M is a good approximation, M‚Åª¬πA has a much smaller condition number.

Common preconditioners:
- **Jacobi**: M = diag(A)
- **Incomplete Cholesky**: M ‚âà LL^T with sparse factors
- **Multigrid**: M is a V-cycle (very effective for PDEs)

Preconditioned CG (PCG) is what you'd actually use in production code. But vanilla CG is already a huge improvement over gradient descent.

## Parallelization

The C implementation parallelizes three operations:

1. **Matrix-vector product** (A*p): Each row can be computed independently.
   ```c
   #pragma omp parallel for
   for (int i = 0; i < N; i++) {
       Ap[i] = Œ£‚±º A[i][j] * p[j];
   }
   ```

2. **Dot products** (r^T r, p^T Ap): Use parallel reduction.
   ```c
   #pragma omp parallel for reduction(+:sum)
   for (int i = 0; i < N; i++) {
       sum += r[i] * r[i];
   }
   ```

3. **Vector updates** (x = x + Œ±*p, etc.): Embarrassingly parallel.
   ```c
   #pragma omp parallel for
   for (int i = 0; i < N; i++) {
       x[i] += alpha * p[i];
   }
   ```

For n = 3, parallelization overhead probably dominates. But for large sparse systems (n = 10‚Å∂), this scales well.

The *iteration* loop itself is sequential‚Äîeach iteration depends on the previous one. But that's fine because most of the work is in the parallel operations within each iteration.

## Implementation Notes

The code uses several helper functions:
- `mat_vec_mul`: Computes y = Ax with `#pragma omp parallel for`
- `dot_product`: Computes x^T y with reduction
- `vec_update`: SAXPY operation (y = y + Œ±*x)

These are the BLAS-level primitives that CG needs. In a real scientific code, you'd use an actual BLAS library (like OpenBLAS or MKL) which has highly optimized versions of these.

## When to Use CG

CG is great when:
- A is symmetric positive definite (SPD)
- A is large and sparse (so you can do matrix-vector products quickly)
- You don't need the exact solution, just an approximation

CG is overkill for:
- Small dense systems (just use Cholesky factorization)
- Indefinite systems (need MINRES or GMRES instead)
- Very ill-conditioned systems without a good preconditioner

For our test case (3√ó3 diagonally dominant matrix), CG is total overkill. But it's a nice proof of concept.

## Compilation and Execution

```bash
gcc -o cg conjugate_gradient.c -fopenmp -lm
./cg
```

The output shows the residual norm at each iteration. For the test system, CG converges in 3 iterations (as expected, since n = 3).
